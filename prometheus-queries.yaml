# Hashicorp VAULT metrics #

avg(vault_identity_num_entities)
# number of identity entities in HashiCorp Vault. In Vault, an identity entity is a unique identity that can be associated with one or more authentication methods. 
# This allows for a unified identity management system within Vault. this is per each namespace. identity entity can be considered mix of user/identity and how it can be authenticated

vault_expire_num_leases
# the number of leases (a lease is a time-bound token or secret that has an expiration time. Leases are used to manage the lifecycle of secrets and tokens) that are currently 
# managed by HashiCorp Vault

avg without(instance) (vault_token_count_by_ttl)
# the count of tokens in HashiCorp Vault, categorized by their time-to-live (TTL). TTL is a duration that specifies how long a token is valid before it expires

avg(irate(vault_token_lookup_count[1m]))
# the count of token lookup operations in HashiCorp Vault. A token lookup operation is typically used to retrieve information about a specific token, such as its policies, TTL, and other metadata, average by all vault pods

avg(vault_runtime_heap_objects{} / vault_runtime_malloc_count{})
# (number of heap objects currently allocated in the memory of the Vault process. Heap objects are dynamically allocated memory blocks) / 
# ( total number of memory allocation (malloc) operations performed by the Vault process),  average number of heap objects per memory allocation operation
# , lower than 0.2 means memory is being used efficiently 

avg(irate(vault_policy_get_policy_count[1m])) 
# count of policy retrieval operations in HashiCorp Vault. In Vault, policies are used to define what actions and operations are permitted for different users and roles. This metric would track the number of times policies have been retrieved or accessed

group by (pod) (up{job="hault"})  # Health Status:  checks if the hault pods are up and working or not (currently only shows existing pods)

max(1 + vault_core_unsealed{}) # Sealed Status: if the vault is unsealed (unlocked) value is 2 otherwise 1 (sealed)

vault_secret_kv_count # Secrets: count of key-value (KV) secrets stored in HashiCorp Vault visible in column Number of entries

vault_route_read_secret_count # Number of Operations in secret: the count of read operations on secrets through specific routes in Vault. In Vault, routes are used to define how requests are handled, and this metric would track the number of times secrets are read via these route

avg(vault_token_count) # Available Tokens: number of active tokens in HashiCorp Vault. In Vault, tokens are used for authentication and authorization. They are issued to clients and can have various properties such as time-to-live (TTL), policies, and renewable status

sum(vault_core_handle_request) by (quantile) # Request Handling: which quantile (p 0.5 , p 0.9 , p 0.99) handles how many requests

avg(vault_token_create_count - vault_token_store_count) # Pending Tokens: count of token creation operations in HashiCorp Vault - count of tokens currently stored in Vault , so you will get how many tokens are created but not saved in Vault

avg(vault_runtime_alloc_bytes{}/1000000)
# number of Megabytes of memory currently allocated and in use by the Vault process, we can view how the memory usage is on average for all 3 Vault pods

vault_audit_log_request_count, vault_audit_log_response_count
# count of audit log requests in Vault. Vault's audit logging feature records all requests and responses to the Vault server, providing a detailed log of 
# all operations for security and compliance purposes. This metric would track the number of audit log entries created

vault_audit_log_request_failure, vault_audit_log_response_failure
# count of failed attempts to log request/responses in the audit log of Vault. Vault's audit logging feature records all requests and responses to the Vault
# server, and this metric would track the number of times an attempt to log a request/response has failed. Failures could occur due to various reasons 
# such as misconfigurations, permission issues, or storage problems.

#=========================================================================

# Kafka Metrics # 

# This part of the query selects the time series data for the specified Kafka consumer group lag over the last 5 minutes.
kafka_consumergroup_lag{consumergroup="borrow-gate", topic="borrow.gateway.balance0.event.triage"}[5m]
# deriv(...): The deriv function is applied to this time series data to compute the rate of change (derivative) per second. It essentially tells you how quickly the consumer group lag is increasing or decreasing over the specified time window.
deriv(kafka_consumergroup_lag{consumergroup="borrow-gate", topic="borrow.gateway.balance0.event.triage"}[5m])
# sum(... by (topic)) : After calculating the derivative, the sum function aggregates the results by the topic label. This means it sums up the rate of change for all instances of the specified topic, providing a single value that represents the overall trend for that topic
sum (deriv(kafka_consumergroup_lag{consumergroup="borrow-gate", topic="borrow.gateway.balance0.event.triage"}[5m])) by (topic)

kafka_burrow_partition_current_offset_vault{aws_account_name="CORE", cluster="vault", container="exporter", env="test", group="borrow-collections", instance="100.72.3.7:13434", job="core-vault/burrow", k8scluster="core-vault", kubernetes_namespace="core-vault", partition="0", pod="burrow-l2gsg", project="kafka", prometheus="monitoring/prometheus", prometheus_replica="prometheus-prometheus-0", quality="full-fat", region="eu-west-2", topic="borrow.refresh-collections-v001.dlq"}

# This PromQL query calculates the rate of change in the current offset for a specific Kafka partition in the borrow-collections consumer group. The rate of change is calculated over a 5-minute window.
alert: PrometheusTSDBCompactionsFailing
expr: increase(prometheus_tsdb_compactions_failed_total{job="prometheus-prometheus",namespace="monitoring"}[3h]) > 0
for: 4h
severity: warning
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} compaction failures over the last 3h.

kafka:topic_consumer_group_lag_by_service{app!="", kubernetes_namespace!=""}

topk(2, kafka_consumergroup_lag{topic=~"core.contracts.journeys.account_schedule.responses"})

group by (topic) ({topic=~".*\\.(dlq|failures)$", k8scluster="core-vault-1", namespace="core"})

sum by (topic, group) (max without (instance) (kafka_burrow_partition_lag{group=~"vault.*|epi.*|schedule.*"}))

# Teams can monitor Kafka consumer lag with the consumer group script, Burrow (a Kafka monitoring companion), or generic monitoring tools and exporters:
group by (app, topic, kubernetes_namespace, consumer_group)
          (kafka_consumer_assigned_partitions_current_gauge or kafka_consumer_batch_processing_time_bucket)
      )
      *
      on(topic, kubernetes_namespace, consumer_group)
      group_left()
      (
        sum by (topic, kubernetes_namespace, consumer_group)
        (
          label_replace(
            kafka_burrow_partition_lag{topic!="__consumer_offsets"},
            "consumer_group",
            "$1",
            "group",
            "(.*)"
          )
        )
      )

kafka_consumergroup_lag

topk(2, kafka_consumergroup_lag{topic=~"v.core.contracts.account_schedule.response"})

topk(2, kafka_consumergroup_lag{consumergroup="v.core.accounts.account_event_processor", topic="v.core.account.event"})

max(kafka_consumergroup_lag) by (service)


UnderReplicatedPartitions
# The number of under-replicated partitions across all topics on the broker. Under-replicated partition metrics are a leading indicator of one or more brokers being unavailable.

IsrShrinksPerSec/IsrExpandsPerSec
# If a broker goes down, in-sync replica ISRs for some of the partitions shrink. When that broker is up again, ISRs are expanded once the replicas are fully caught up.

ActiveControllerCount
# Indicates whether the broker is active and should always be equal to 1 since there is only one broker at the same time that acts as a controller.

LeaderElectionRateAndTimeMs
# A partition leader election happens when ZooKeeper is not able to connect with the leader. This metric may indicate a broker is unavailable.

BytesInPerSec/BytesOutPerSec
# The number of data brokers received from producers and the number that consumers read from brokers. This is an indicator of the overall throughput or workload in the Kafka cluster.

RequestsPerSecond
# Frequency of requests from producers, consumers, and subscribers

CollectionCount
# The total number of young or old garbage collection processes performed by the JVM

CollectionTime
# The total amount of time in milliseconds that the JVM spent executing young or old garbage collection processes

compression-rate-avg
# Average compression rate of sent batches

response-rate
# An average number of responses received per producer

request-rate
# An average number of responses sent per producer

io-wait-time-ns-avg
# The average length of time the I/O thread spent waiting for a socket (in ns)

records-lag
# The number of messages consumer is behind the producer on this partition

records-lag-max
# The maximum number of messages consumer is behind the producer on this partition

records-consumed-rate
# An average number of records consumed per second for a specific topic or across all topics

avg-latency
# The response time to a client request is in milliseconds

# provides the average time taken for a fetch request
kafka_consumer_fetch_manager_metrics_fetch_latency_avg

# provides the average number of responses received by the producer per second
kafka_producer_producer_metrics_response_rate

# provides the total time taken to process requests
kafka_network_requestmetrics_totaltimems

# kafka_consumergroup_current_offset => latest offset that a Kafka consumer group has processed. An offset in Kafka is a unique identifier of a record within a partition. It denotes the position of the consumer in the partition. For example, if a consumer is at offset 5, it means the consumer has read records 0 through 4 and is ready to read record 5.
# Monitoring this metric can provide valuable insights into your Kafka consumer groups' behavior and performance. For instance, if the current offset stops increasing for a particular consumer group, it might indicate that the consumers in that group are no longer processing records, which could signal a problem.
sum(increase(kafka_consumergroup_current_offset{consumergroup="rewards-campaign-manager"}[30d]))

increase(kafka_network_requestmetrics_errors_total{kubernetes_name="kafka",kubernetes_namespace=~"core-v" ,kubernetes_cluster=~"core-v.dynamo.prod.eu-west-1.aws.mycorps.net",error!="NONE"}[7d]) > 1

sum(max(kafka_burrow_partition_lag{topic=~"v.core.audit.creation_requests|v.core.audit.logs|v.core.audit.action_log.creation_request", group=~"v.audit.audit_processor|v.audit.audit_l2_processor|v.audit.action_log_processor", kubernetes_cluster=~"p-vt.prod.eu-west-2.aws.mycorps.net", kubernetes_namespace=~"core-vt"}) without (instance)) by (group)

sum by(topic, group)(
    max without(instance)(
        kafka_burrow_partition_lag{
            kubernetes_namespace=~"core-v", 
            kubernetes_cluster="core-v.dynamo.prod.eu-west-1.aws.mycorps.net", 
            topic=~"vt.core.contracts.contract_notifications.event", 
            group=~".*"  # wildcard to accept all groups
        }
    )
# Further filters the final result to only include a specific consumer group and topic
) {group="v.v2.core_streams_api", topic="v.core.contracts.contract_notifications.event"}


count by (topic)(
    count by (topic, partition)(
        kafka_cluster_partition_replicascount{
            kubernetes_name="kafka", 
            kubernetes_namespace=~"core-v", 
            kubernetes_cluster=~"p-v.prod.eu-west-2.aws.mycorps.net", 
            topic=~".*"
        }
    )
)


# The metric refers to the rate of fetch requests per second that are being served by follower replicas in a Kafka cluster. 
# => kafka server, broker topic metrics, fetch from follower, fetch request per sec
kafka_server_brokertopicmetrics_fetchfromfollowerfetchrequestspersec

ALERTS{alertname="NewMessagesInMigratedPostingsBridgeDLQ", alertstate="firing", topic="vt.api.v1.postings.posting_batch.created.migrated_postings_bridge.dlq"}

=========================================

name: capquest_updates_kafka_takes_too_long
type: lag
consumergroup: borrow-gateway
topics: tokenized.client-individual-customer-event-v02-03
threshold: 0
agg_function: min()
agg_window: 5m
message: Consumer lag for customer event topic in borrow-gateway has been increasing for 20 minutes - fa-gateway is taking too long to process events - 
         customer details updates are not being sent over to Capquest in a timely manner

#=========================================================================

# What is Telemetry
# Telemetry in software refers to the collection of business and diagnosis data from the software in production and store and visualize it for the purpose of diagnosing production
# issues or improving the business

# How many errors and exceptions?
# what is response time?
# how many times api is called?
# how many servers?
# How many users in New York?

# prometheus stores data as time series
# every time series is identified by metrics name and label
# labels are a key and value pair
# labels are optional

# Node Exporter
# every unix-based kernel/computer is called a Node. node exporter is an official prometheus exporter for collecting metrics that are exposed by unix-based kernels (linux/ubuntu)
# example of metrics are CPU, disk, memory, network i/o , Node exporter can be extended with pluggable metric collectors.
# prometheus should run on a separate server or docker image than our application.

# Prometheus runs on port "9100" by default
# LOG (Metric, Value, Time) ==> Time series database  ==> Prometheus (Store, Query, Alert)
# The targets that we scrape via prometheus, it includes the prometheus itself

<metric name> {key=value, key=value}
auth_api_hit {count=1, time_taken=800}

#Data types in promQL
#Scalar : float , String

prometheus_http_requests_total{code="200", job="prometheus"}

prometheus_http_requests_total{code =~"2.*", job="prometheus"}

# Instant vectors => instant vector selectors allow the selection of a set of time series
# and a single sample value for each at a given timestamp
# only a metric name is specified
# result can be filtered by providing labels

auth_api_hit 5
auth_api_hit{count=1, time_taken=800} 1

# Range Vector => range vectors are similar to instant vectors except they select 
# a range of samples

ms, s, m, h, d,w, y

label_name[time_spec]
Auth_api_hit[5m]

# operators => addition + subtraction - multiplication * division / Modulo %  Power ^
scrape_sample_post_metric_relabeling - 100

A          B            A+5          A+B
M1{a} 3    M1{a} 37     M1{a} 8      M1{a} 40
M1{b} 4    M1{b} 6      M1{b} 9      M2{b} 10 
M1{c} 9                 M1{c} 14     -


M1{a} 10    
           == 10      =>   M1{a} 10
M1{b} 4                    M1{b} Removed


# Comparison operators  =>  ==  != >  <  >=  <=
process_cpu_seconds_total{instance="localhost:9182"}

# filter matchers/selectors
                               # AND
<metric name> {filter_key=value , filter_key=value,...}

#=~   value on the left must match the regular expression (regex) on the right
#!~   value on the left must NOT match the regular expression (regex) on the right
process_cpu_seconds_total{job!="node_exporter"}

prometheus_http_requests_total{code=~"2.*", job="prometheus"}

prometheus_http_response_size_bytes_bucket{handler=~"/static/.*"}

# group => groups elements, all values in the resulting vector are equal to 1
# count_values => counts the number of elements with same values
# topk (n, vector)
# bottomk(n, vector)
# stddev => finds population standard deviation over dimensions

# absent(<instant vector>) : checks if an instant vector has any members, returns empty vector if parameter has elements
# absent_over_time(<instant vector>) : checks if an instant vector has any members, returns empty vector if parameter has elements over a period of time
# abs(<instant vector>) : -5 to 5
# ceil(<instant vector>) : 1.6 to 2
# floor(<instant vector>) : 1.6 to 1
# clamp(<instant vector>, min, max) : 1.6 to 1.9

# delta<instant vector>) => can be used over gauges
# idelta(<instant vector>) => returns the difference between first and last item
# delta(node_cpu_temp2[h])

# <Aggregation Operator> (instant vector)
sum(node_cpu_total)

#<Aggregation Operator> (instant vector by <label list> )
sum(cpu_node_total) by (http_code)

# <Aggregation Operator> (instant vector without <label list>)
sum(cpu_node_total) without (http_code)

topk(3, cpu_time_total)

# the results will always be ONE for each group
group(cpu_node_total) by (mode)

group by (uri) (http_client_requests_seconds_bucket{service="account-closure-service"})

group by (client_name) (http_client_requests_seconds_bucket{service="account-closure-service"})

group(istio_request_bytes_bucket{service="payment-transfer-service", le="+Inf"}) by (destination_service_name)

# average of each group
avg(windows_cpu_time_total) by (mode)

# Offset, delays how we capture the instant vector and goes back in time by the offset value
prometheus_http_requests_total offset 15m

# how many records we have in each response_code group
count(prometheus_http_requests_total offset 1h) by (code)

# how many different metrics are available for this specific service
count by (__name__) ({service="eligibility-service", le="+Inf"})

# checks if an instant vector has any members, returns empty vector if it has elements
absent(process_cpu_seconds_total{job="node_exporter"})

# returns values with minimum of 100 and above
clamp_min(process_cpu_seconds_total, 100)

day_of_week(cpu_time_total)

# returns results from Max to Min record
sort_desc(auth_api_hit{time_taken=500})

# avg_over_time(<range vector>) : returns the average of items in a range over time
# sum_over_time(<range vector>)
# min_over_time(<range vector>)
# max_over_time(<range vector>)
# count_over_time(<range vector>) : returns the count of items in a range over time

avg_over_time(windows_cpu_cstate_seconds_total{state=”c1”}[10h])

# returns the averge of the metrics over 10 hours period
avg_over_time(prometheus_http_requests_total[10h])

# Raw counters by default do not mean much as they ALWAYS go UP
demo_api_requests_duration_seconds_count  # will always increase

# Rated counters are much better at capturing the increase or decrease over a defined time window
rate(demo_api_requests_duration_seconds_count[5m])

# rate(), irate(), increase() all consume a time ranged raw counter and return the change rate over that window
# The input window [15m] has to have at least two points inside it to be able to return result in rated counter functions

# rate() => pe-second, smoothed
rate(node_cpu_seconds_total{mode!="idle"}[5m])

# irate() => is per-second, but non-smoothed
# you can find a lot of up and downs in irate chart
irate(node_cpu_seconds_total{mode!="idle"}[5m])

# increase() => absolute value, smoothed
# increase has same chart shape as rate, but with different(real) values
increase(node_cpu_seconds_total{mode!="idle"}[5m])

sum(increase(http_server_requests_seconds_count{app="borrow-gateway", uri="/api/v1/financial-assistance/gateway/customer/{customerId}/status/{statusId}", method="PUT", status!~"2.*"}[2h]))

# rate() and increase() only calculate the values between first and last item in the time range and do not care about values in the middle of range
# Rate and Increase give back different values but the graphs will look the same at the end.

# increase(v range-vector) calculates the increase in the time series in the range vector. The increase is extrapolated to cover the full time range as specified in the range vector selector, 
# so that it is possible to get a non-integer result even if a counter increases only by integer increments.

increase(http_requests_total{job="api-server"}[5m])

increase(kube_pod_container_status_restarts_total[15m]) > 3  # show only records for pods that have been reset more than 3 times

# Offset can be used in calculation for time comparison
demo_memory_usage_bytes offset 1w - demo_memory_usage_bytes

demo_memory_usage_bytes{instance=demo-service-0:10000"}[1m] offset 1h

# Summaries => track distribution of request latencies or some other set of numeric values. Summary can NOT be aggregated.
http_requests_duration_seconds{quantile="0.5"}
http_requests_duration_seconds{quantile="0.90"}
http_requests_duration_seconds{quantile="0.99"}
http_requests_duration_seconds_sum
http_requests_duration_seconds_count

# this is wrong, since summaries can't be aggregated
avg by (job) (http_requests_duration_seconds{quantile="0.5"})

{app="loan-manager || loan-manager-api"}

# Le: Less or equal
# Ge : greater or equal

# Histogram_quantile
# histogram_quantile(<target_quantile>, <histogram>)

http_requests_total  # time series with total of http requests

http_requests_total{job="apiserver", hanlder="/api/comments"}  # for job that is apiserver and handler that is /api/comments give the total http requests

http_requests_total{job="apiserver", hanlder="/api/comments"}[5m]  #  same as above but for the range of last 5 minutes

http_requests_total{environment=~"staging|testing|development",method!="GET"}  # we can have one of several environments and method NOT "GET"

http_requests_total{job=~".*server"}  # for all jobs that end with "server" give back http_requests_total metric

http_requests_total{status!~"4.."}  # select all HTTP status codes except 4xx ones

rate(http_requests_total[5m]) # Return the per-second rate for all time series with the http_requests_total, measured over the 5 minutes

sum by (job) (rate(http_requests_total[5m])) # sum up all the rates over last 5 minutes, grouping by job value

rate(http_requests_total[5m])[30m:1m]  # Return the 5-minute rate of the http_requests_total metric for the past 30 minutes, with a resolution of 1 minute

topk(3, sum by (app, proc) (rate(instance_cpu_time_ns[5m])))  # we could get the top 3 CPU users grouped by application (app) and process type (proc) 

sum by (namespace) (kube_pod_info) # total pod count per namespace

sum(kube_pod_container_resource_limits{resource="cpu"}) - sum(kube_node_status_capacity{resource="cpu"}) # cpu overcommitment in the cluster

# min_over_time(range-vector)  =>  the minimum value of all points in the specified interval

# Find unhealthy Kubernetes pods
min_over_time(sum by (namespace, pod)
    (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})[15m:1m]) > 0 

# 90th percent latency, averaged over the last 5 minutes
histogram_quantile(
  0.9, 
  rate(http_requests_duration_seconds_bucket[5m])
)

# 90th percent quantile for latency for each path, method, le combination
# averaged over last 5 minutes
histogram_quantile(
  0.9,
  sum by(path, method, le) (
    rate(http_requests_duration_seconds_bucket[5m])
  )
)

histogram_quantile(0.95, sum by(le, path, method) (rate(demo_api_request_duration_seconds_buckets[5m]))

# average request duration over the last 5 minutes
# total_value_summed / count
rate(http_requests_duration_seconds_sum[5m])
/
rate(http_requests_duration_seconds_count[5m])

# aggregated per-path/method average request duration
# The sum by (path, method) groups the results by path and method, summing up the rates for each unique combination
sum by (path, method) (rate(http_requests_duration_seconds_sum[5m]))
/
sum by (path, method) (rate(http_requests_duration_seconds_count[5m]))

# sum by => means to aggregate all the data into different times series based on how many kinds we have

sum by (instance) (rate(demo_cpu_usage_seconds_total{mode!="idle"}[5m]))
/
# query is trying to perform a PromQL join using the on (instance) group_left() clause with demo_num_cpus
# on (instance) => This ensures that the join happens only where the instance label matches in both metrics
# group_left() => By default, Prometheus joins only one-to-one or many-to-one relationships, Since demo_num_cpus has additional labels beyond instance, group_left() allows the right-side metric (demo_num_cpus) to keep its extra labels
on (instance) group_left() demo_num_cpus


# what is difference between rate and deriv functions?

# The rate() and deriv() functions in Prometheus are both used to analyze the rate of change of time series data, but they are used in different contexts and have different purposes:
# 1. rate() Function:
# Purpose: The rate() function is specifically designed for calculating the per-second average rate of increase of a counter over a specified time range. It is typically used with metrics that are counters, which are monotonically increasing values that reset to zero upon restart.
# Use Case: It's commonly used for metrics like request counts, error counts, or any other metric that accumulates over time.
# Behavior: rate() calculates the average rate of increase by looking at the difference between the start and end values of the counter within the specified time window and dividing by the duration of the window.
# 2. deriv() Function:
# Purpose: The deriv() function calculates the per-second "derivative of a time series", which can be used for both counters and gauges. It estimates the instantaneous rate of change at each point in time.
# Use Case: It's useful for metrics that can both increase and decrease, such as gauges, or when you want to understand the trend or slope of a metric over time.
# Behavior: deriv() uses linear regression to estimate the slope of the time series data over the specified time window, providing a more precise measure of the rate of change at each point.
# In summary, use rate() when dealing with counters to get the average rate of increase, and use deriv() when you need to calculate the instantaneous rate of change for any type of time series, including gauges.


# rate() is like a counting toy:
# Imagine you have a toy that counts how many times you jump up and down. Every time you jump, the number goes up. If you want to know how fast you're jumping, you can look at how many jumps you did in a certain amount of time, like in one minute. That's what rate() does—it tells you how fast the number is going up.

# deriv() is like a magic slope toy:
# Now, imagine you have a toy car on a track that can go up and down hills. Sometimes it goes up, and sometimes it goes down. If you want to know how steep the hill is at any moment, you use the magic slope toy. It tells you if the car is going up or down and how steep the hill is. That's what deriv() does—it tells you how quickly something is changing, whether it's going up or down.
# So, rate() is for counting how fast something is going up, and deriv() is for figuring out how steep the change is, whether it's going up or down.

# In PromQL, the regular expression status=~"50.+" is used to match status codes that start with "50" followed by one or more characters. To modify this expression to also accept values like "511" or "519", you can adjust the regular expression to match any status code starting with "5" followed by two digits. Here's how you can do it:
status=~"5[0-9][0-9]"
status=~"5.+"

avg by (operation) (rate(service_requests{namespace="105250-temporal", service_name="frontend"}[5m]))

sum by (activity_type) (rate(temporal_activity_execution_latency_seconds_count{namespace=~"borrow", exported_namespace="borrow"}[1m]))


temporal_workflow_endtoend_latency_seconds_sum{workflow_type=~"CardEligibilityflow|CreditCardApplyflow"}

temporal_activity_succeed_endtoend_latency_seconds_bucket{activity_type=~"SendEligibilityResponse|CheckEligibilityResponse", workflow_type=~"UKCreditWorkflow|UKCreditligibilityWorkflow",task_queue="CREDIT_ELIGIBILITY_PUBLISH_RESPONSE_TASK_QUEUE"}

temporal_workflow_endtoend_latency_seconds_count{workflow_type=~"CardEligibilityWorkflow|CreditCardApplyflow"}

sum by (workflow_type, task_queue) (rate(temporal_workflow_task_queue_poll_succeed_total{namespace=~"borrow", exported_namespace="borrow"}[1m]))

histogram_quantile(0.99, sum(rate(syncmatch_latency_bucket{namespace="borrow-temporal"}[5m])) by (operation, le, type))

# This PromQL query calculates the 95th percentile latency for a given service and operation in the borrow-temporal namespace using histogram buckets
# sum(rate(...[1m])) by (operation, le)  => Computes the per-second rate of the histogram bucket counts over a 1-minute window Groups by operation and le (the bucket upper bounds for latency)
# histogram_quantile(0.95, ...)  => Calculates the 95th percentile latency for each operation based on the histogram buckets
histogram_quantile(0.95, sum(rate(service_latency_bucket{namespace="borrow-temporal", service_name=~"my-svc", operation!~"PollActivityTaskQueue|PollWorkflowTaskQueue"}[1m])) by (operation, le))

# time elapsed since the last completion of a Kubernetes CronJob (borrow-temporal-rds-rotation-cron) in the borrow-temporal namespace
# max(...) ensures that we get the most recent completion time
# This query is useful for monitoring scheduled jobs, ensuring that they run on time.
time() - max(kube_job_status_completion_time{namespace='borrow-temporal', job_name=~'borrow-temporal-rds-rotation-cron.+'}))

# automated format to create grafna dashboard and alerts
consumergroup: borrow-gateway
topics: ods-transaction-enrichment-service.transaction-enrichment-event-v1
threshold: 0
agg_function: min()
agg_window: 5m
alert:
  severity: MINOR
  for: 20m
  message: Consumer lag for transaction event topic has been increasing for 20 minutes - gateway is taking too long to process ODS transaction events - customer transactions are not being sent over in a timely manner
global_tags:
  grafana_notification_channel: Collections-Squad


error_query: sum(rate(http_server_requests_seconds_count{app="balance-service", uri=~"/api/v1/balance/(accounts/live-balances|internal/live-balance|live-balance-views|accounts/timerange-balance)", method="GET", status!~"2.*|3.*"}[5m]))
total_query: sum(rate(http_server_requests_seconds_count{app="balance-service", uri=~"/api/v1/balance/(accounts/live-balances|internal/live-balance|live-balance-views|accounts/timerange-balance)", method="GET"}[5m]))
objective: 0.99


error_query: sum(rate(http_server_requests_seconds_count{app="borrow-gateway", uri="/api/v1/borrow/assistance/gateway/customer/{customerId}/status/{statusId}", method="PUT", status=~"2.*"}[5m])) 
             - 
             sum(rate(http_server_requests_seconds_bucket{app="borrow-gateway", uri="/api/v1/borrow/assistance/gateway/customer/{customerId}/status/{statusId}", method="PUT", status=~"2.*", le=~"8.*"}[5m]))
total_query: sum(rate(http_server_requests_seconds_count{app="borrow-gateway", uri="/api/v1/borrow/assistance/gateway/customer/{customerId}/status/{statusId}", method="PUT", status=~"2.*"}[5m]))
objective: 0.99


# how can i create a custom promql function/expression?

# Recording Rules:
# You can create recording rules in Prometheus to precompute and store the results of complex expressions. This is useful for frequently used queries that are computationally expensive.
# Define a recording rule in your Prometheus configuration file under the rule_files section. For example:

groups:
- name: custom_rules
  rules:
  - record: job:http_inprogress_requests:sum
    expr: sum by (job) (http_inprogress_requests)
# This rule will compute the sum of http_inprogress_requests by job and store it as a new time series job:http_inprogress_requests:sum

# Prometheus Exporters:
# If you need to perform custom calculations that are not possible with PromQL, consider writing a custom Prometheus exporter. Exporters can expose metrics in a format that Prometheus can scrape, and you can implement any custom logic you need in the exporter code.

# External Processing:
# Use external tools or scripts to process Prometheus data. You can use the Prometheus HTTP API to fetch raw data and then process it using a programming language of your choice (e.g., Python, Go).
# This approach allows for complex calculations and custom logic that are not feasible directly in PromQL.

# PromQL Extensions:
# If you are comfortable with Go programming and want to extend Prometheus itself, you can modify the Prometheus source code to add new functions to PromQL. This is an advanced approach and requires maintaining your custom version of Prometheus.

# Grafana Transformations:
# If you are using Grafana to visualize Prometheus data, you can use Grafana's transformation features to manipulate data after it has been queried from Prometheus.

# prometheus_target_scrapes_sample_duplicate_timestamp_total => This metric counts the number of samples with duplicate timestamps observed during a scrape, Duplicate timestamps usually indicate that a target is exposing the same timestamp for multiple samples, which can lead to issues in data storage and querying
# rate(...[5m])  => Computes the per-second rate of duplicate timestamp occurrences over the last 5 minutes
# > 0  => Triggers when at least one duplicate timestamp occurrence per second is detected in the 5-minute window
# expr:  => The expr: prefix is commonly used in Prometheus alerting rules or monitoring configurations, such as in PrometheusRules (for Alertmanager) or in some monitoring dashboards, It defines the PromQL expression that should be evaluated
expr:rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus-prometheus",namespace="monitoring"}[5m]) > 0

sum by(kubernetes_namespace)(increase(scheduler_job_publish_lag_seconds_count{kubernetes_cluster="my-cluster", kubernetes_namespace=~"my-ns"}[2m]))

#  this only works if kube_pod_status_phase has values 0 and 1 (where 1 means a pod is running). This is common in some Kubernetes metric implementations
count(kube_pod_status_phase{phase="Running", kubernetes_cluster="icore.eng.eu-west-2.aws.server.net"} == 1)

sum(ALERTS{alertstate="firing"}) # how many alerts are triggered and firing

# count the number of "Ready" nodes in a Kubernetes cluster
# max without(instance)(...)  => Aggregates the metric by removing the instance label, This ensures that each node contributes a single value (eliminating duplicate metrics from different Prometheus instances).
# sum(...)  => Computes the total count of nodes that are "Ready"
sum(max without(instance)(kube_node_status_condition{kubernetes_cluster="my-cluster", condition="Ready", status="true"}))

# a metric name used in Prometheus for monitoring HTTP server requests. It's a histogram metric type that provides a distribution of the duration of HTTP requests over a defined set of intervals, known as buckets.
# The metric http_server_requests_seconds_bucket has a label le that represents the upper bound of the bucket. The value of the le label is the inclusive upper bound of the bucket.
# This metric allows you to monitor the performance of your HTTP server and understand the distribution of request durations
http_server_requests_seconds_bucket{service=""}

# a metric name used in Prometheus for monitoring HTTP client requests. It's a histogram metric type that provides a distribution of the duration of HTTP requests over a defined set of intervals, known as buckets.
# This metric allows you to monitor the performance of your HTTP client and understand the distribution of request durations
http_client_requests_seconds_bucket{service=""}

sum by(app, uri, le) rate(http_server_requests_seconds_bucket{app="movemoney-card-decision", uri=~"/api/.+"}[1m])

# how many 500x errors percentage
sum(rate(http_server_requests_seconds_count{app="movemoney-payment", status =~"5.*"} [5m])) 
/ 
sum(rate(http_server_requests_seconds_count{app="movemoney-payment"} [5m])) * 100

sum(rate(http_server_requests_seconds_count{app="movemoney-engine-service"}[1m])) by (uri)

sum(irate(istio_requests_total{connection_security_policy!="mutual_tls", reporter="source", destination_service_name="movemoney-issuer-processor"}[5m])) by (source_workload)

sum(rate(http_server_requests_seconds_count{status!~"2.*", app=~"movemoney-engine-service", uri=~"/api/.+"} [1m])) by (uri, pod, app, status)

# This is a metric that measures the duration of HTTP requests in seconds. It's a histogram metric where the le label indicates the upper bound of the histogram bucket
http_server_requests_seconds_bucket{app="borrow-manager", uri="/api/v1/borrow/revolving-line", method="PUT", status="200", le="+Inf"} 

# calculate percentage of scheduler operations that happens on le=1 lag time
sum(rate(scheduler_db_ops_duration_bucket{le="1", type="read", kubernetes_cluster=~"p-vt.*", kubernetes_namespace="core-vt"}[5m])) by (kubernetes_namespace) 
/
sum(rate(scheduler_db_ops_duration_count{kubernetes_cluster=~"p-vt.*", kubernetes_namespace="core-vt"}[5m])) by (kubernetes_namespace)


# get rate of successful responses by its url and method (get, post,...) in total sum
histogram_quantile(0.95, sum(rate(http_client_requests_seconds_bucket{app="borrow-manager", client_name=~"corebanking-manager.*", status=~"2.*"}[2m])) by (le, uri, method))

sum by (outcome) (increase(http_client_requests_seconds_bucket{app="corebanking-manager", client_name=~"core-gateway.*", le="+Inf"}[2m]))

# This gives a single average value across all zones
avg (
  # Counts the number of pods grouped by availability zone, label_failure_domain_beta_kubernetes_io_zone is the label that represents the zone where the pod is running
  count by (label_failure_domain_beta_kubernetes_io_zone) (
    # project:app:pod_zone is a custom metric that tracks the number of pods in each failure domain (availability zone)
    project:app:pod_zone{kubernetes_cluster="my-cluster", namespace=~"my-ns"}
  )
)

# sum without(instance, pod) (...) => The sum function is an aggregator that sums up all the values in the specified range. The without clause removes the listed labels (instance and pod in this case) from the result. So, this part of the query is summing up all the values of the filtered metric, but it's doing so in a way that doesn't distinguish between different instances or pods. This means that the result will be the total count of HTTP requests matching the specified criteria across all instances and pod
sum without(instance, pod) (http_server_requests_seconds_bucket{app="borrow-manager", uri="/api/v1/borrow/revolvings", method="PUT", status="200", le="+Inf"})


sum by (app) (100 * rate(PostingsProcessor_StageBatchRejectionCounter{kubernetes_cluster="core-vt"}[10m]))
/
sum by (app) (rate(PostingsProcessor_BatchRequestCounter{kubernetes_cluster=~"core-vt"}[10m]))

# Estimates the 99th percentile latency (P99) from the histogram
# This means 99% of queries finish faster than this value, and the slowest 1% take longer
histogram_quantile(0.99, 
  # le (the upper bound of latency buckets
  sum by (le, kubernetes_namespace, app) (
    # Calculates the per-second rate of observations over a 10-minute window
    rate(database_client_query_duration_seconds_bucket{project="vault", service!="watermark-processor"}[10m])
  )
)

# histogram_quantile(φ scalar, b instant-vector) => calculates the φ-quantile (0 ≤ φ ≤ 1) from a classic histogram or from a native histogram. 
# The float samples in b are considered the counts of observations in each bucket of one or more classic histograms. Each float sample must have a label le where the label value denotes the inclusive upper bound of the bucket.

# gives you metric and its labels based on how often the app is scraped
tl_workflow_endtoend_latency_seconds_bucket{app="borrow-collections"} 

# divides the vector results into 15m intervals and calculates the rate for each of them
rate(tl_workflow_endtoend_latency_seconds_bucket{app="borrow-collections"}[15m]) 

# sums up all of rate of the metric over the 15m intervals
sum(rate(tl_workflow_endtoend_latency_seconds_bucket{app="borrow-collections"}[15m])) 


# total number of pods in each namespace that have cpu limits
count by (namespace)(sum by (namespace, pod, container)(kube_pod_container_resource_limits{resource="cpu"}))  

# Find the number of containers without CPU limits in each namespace
count by (namespace)(sum by (namespace, pod, container)(kube_pod_container_info{container!=""}) unless sum by (namespace, pod, container)(kube_pod_container_resource_limits{resource="cpu"}))

# Find PersistentVolumeClaim in the pending state
kube_persistentvolumeclaim_status_phase{phase="Pending"}  

# count them in each namespace
count by (namespace)(kube_persistentvolumeclaim_status_phase{phase="Pending"})

# find nodes that are intermittently switching between “Ready" and “NotReady" status continuously over 15 minutes
sum(changes(kube_node_status_condition{status="true", condition="Ready"}[15m])) by (node) > 2 


# what is the difference between "http_server_requests_seconds_bucket" and "http_server_requests_seconds_count" in promql?

# In Prometheus, when dealing with metrics related to HTTP server requests, you might encounter metrics like http_server_requests_seconds_bucket and 
# http_server_requests_ seconds_count. These are typically part of a histogram metric type, which is used to observe the distribution of request durations.

# 1. http_server_requests_seconds_bucket :
# • This metric represents the count of requests that have a duration less than or equal to a specific bucket boundary.
# • Histograms in Prometheus are defined with multiple buckets, each representing a range of values. For example, you might have buckets for request durations 
# like 0.1s, 0.5s, 1s, etc.
# • The http_server_requests_seconds_bucket metric will have a label called le (less than or equal), which indicates the upper bound of the bucket. For example, 
# le="0.5" would count all requests that took 0.5 seconds or less.
# • This metric is useful for calculating quantiles, such as the 95th percentile of request durations.

# 2. http_server_requests_seconds_count :
# • This metric represents the total count of all observed requests, regardless of their duration.
# • It is essentially the sum of all the counts in the
# http_server_requests_seconds_bucket metrics for all bucket boundaries.
# • This metric is useful for calculating the total number of requests over a period of time.
# In summary, http_server_requests_seconds_bucket is used to understand the distribution of request durations across different buckets, while 
# http_server_requests_seconds_count provides the total number of requests observed. Together, they help in analyzing the performance and behavior of HTTP server requests.


# Find idle memory
sum(
  (
    container_memory_usage_bytes{container!="POD", container!=""} # Retrieves the memory usage of running containers, excluding non-container-specific metrics.
    - 
    on(namespace, pod, container)
    avg by (namespace, pod, container) (kube_pod_container_resource_requests{resource="memory"})  # Retrieves the requested memory for each container
  ) * -1 > 0  # Filters out cases where memory usage is within or below the requested amount (i.e., only containers exceeding their request are considered)
) 
/ (1024 * 1024 * 1024)

# average memory usage by pods in each namespace
avg(container_memory_working_set_bytes{container!="POD", container!=""}) by (namespace)

# find number of nodes that are ready
sum(kube_node_status_condition{condition="Ready", status="true"}) 

# fetch the number of allocated file descriptors for a specific Kubernetes cluster named "my-cluster"
node_filefd_allocated{k8scluster="my-cluster"} 

# This is a metric exposed by the Node Exporter, a Prometheus exporter for hardware and OS metrics. It represents the number of file descriptors currently allocated by the system
node_filefd_allocated

# this example will return timeseries with the values "a:c" at label service and "a" at label "foo"
label_replace(up{job="api-server",service="a:c"}, "foo", "$1", "service", "(.*):.*")

sum by(kubernetes_namespace)(
  increase(scheduler_job_publish_lag_seconds_count{kubernetes_cluster=~"dn-p-v.*", kubernetes_namespace=~"250-core-v"}[5m])
)

sum by (cronjob) (kube_job_status_succeeded{namespace="your-namespace", cronjob="your-cronjob-name"})
# This query will give you the total number of successful completions for all Jobs created by the specified CronJob in the given namespace. By monitoring 
# this over time, you can determine if the CronJob is running successfully.

# This PromQL query calculates the success rate of database client statements over a specified lookback_window
# success / total
# This query helps monitor database reliability by showing the proportion of successful queries in a given time window. If the success rate drops, it may indicate failing queries or database issues.
(sum(rate(database_client_statement_duration_seconds_count{
    app=~"$app", 
    outcome="success", 
    name=~"$name", 
    operation=~"$operation", 
    tables=~"$tables", 
    kubernetes_cluster=~"$cluster", 
    kubernetes_namespace=~"$namespace"
}[5m])) 
    /
sum(rate(database_client_statement_duration_seconds_count{
    app=~"$app", 
    name=~"$name", 
    operation=~"$operation", 
    tables=~"$tables", 
    kubernetes_cluster=~"$cluster", 
    kubernetes_namespace=~"$namespace"
}[5m])) 
)

# represents the number of database client statements executed. This metric is usually associated with monitoring database query execution, tracking the count of queries over time
sum(rate(database_client_statement_duration_seconds_count{app=~".*", outcome="success", name=~".*", operation=~".*", tables=~".*", kubernetes_cluster=~"p-v.dynamo.prod.eu-west-1.aws.mycorps.net", kubernetes_namespace=~"core-v"}[7d])) 
empty 

# the increase() function, that will calculate the difference between the counter values at the start and at the end of the specified time interval. It also correctly handles counter resets during that time period
increase(http_requests_total[24h]) # calculates the increase in the total number of HTTP requests over the last 24 hours

histogram_quantile(0.5, sum(rate(grpc_client_connection_rpc_duration_bucket{app="v-account-processor",project="vt",kubernetes_namespace=~"core-v"}[90d])) by (le))  # 0.00861345529

sum by (kubernetes_namespace, processor) (rate(data_exporter_handler_process_time_attempt_seconds_count{app=~"(kernel|vault)-data-exporter", successful="false", kubernetes_cluster="p-v.dynamo.prod.eu-west-2.aws.mycorps.net"}[90d]))


sum(galley_istio_networking_destinationrules{kubernetes_cluster=~"p-v.prod.eu-west-2.aws.mycorps.net"}) 
  / 
count(up{job=~"galley|istio-galley", kubernetes_cluster=~""p-v.prod.eu-west-2.aws.mycorps.net"})

# This query checks if any application container in the core-vt namespace has experienced more than one OOM (Out of Memory) event in the specified Kubernetes cluster
project:app:oom{  #  tracks OOM kill events for application containers
  container=~".*",
  label_app=~".*",
  namespace=~"core-vt",
  kubernetes_cluster=~"p-vt.prod.eu-west-1.aws.mycorps.net"
} > 1


sum(
    up{
        kubernetes_cluster=~"p-vt.prod.eu-west-1.aws.mycorps.net",
        service=~".*",
        namespace=~"core-vt",
        container=~"prometheus"
      }
    ) by (job, service, namespace)

# This is a histogram bucket metric that tracks policy evaluation latencies, The _bucket suffix means this is a cumulative bucket in a histogram
sum( # Computes the total count of policy evaluations that completed within the 10µs bucket.
    policy_engine_evaluate_policies_duration_bucket{
        le=~'1e-05', 
        kubernetes_namespace=~'core-vt', 
        kubernetes_cluster=~'p-vt.prod.eu-west-1.aws.mycorps.net', 
        kubernetes_pod=~'.*'
    }
) by (action, outcome)


# ** alert example **
# if any backend has more than 0.5 connection errors in duration of 5 minutes then fire an alert. this has to continue for at least 10 minutes before alert starts
groups:
- name: Errors
  rules:
  - alert: ErrorsCountIncreased
    expr: rate(haproxy_connection_errors_total[5m]) by (backend) > 0.5
    for: 10m
    labels:
      severity: page
    annotations:
      summary: High connection error count in {{ $labels.backend }}


# Metric Server
# The Kubernetes Metrics Server is an aggregator of resource usage data in your cluster, and it isn't deployed by default in Amazon EKS clusters.
# The Kubernetes Metrics Server is a resource metrics monitoring tool for Kubernetes. The Kubernetes Metrics Server measures CPU and memory usage across the Kubernetes 
# cluster. This metrics-server collects metrics and then shows resource usage statistics for your cluster so that you can monitor resource usage for each node and also for 
# each pod.You can monitor your resource usage with simple commands using the top command like "kubectl top pods" and "kubectl top nodes".

# Install metrics server on cluster:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Check that it works:
kubectl get deployment metrics-server -n kube-system
kubectl top nodes
kubectl top pods

# ** All secrets are saved in AWS SECRET MANAGER
# ** PI should not be visible in application logs

# get metrics from metrics-server directly from kubernetes api
kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/namespaces/456321-core-vt/deployments.apps/vt-postings-account-processor/max_consumer_group_lag"

# {"kind":"MetricValueList","apiVersion":"custom.metrics.k8s.io/v1beta1","metadata":{},"items":[{"describedObject":{"kind":"Deployment","namespace":"456321-core-vt"
# ,"name":vt-postings-account-processor","apiVersion":"apps/v1"},"metricName":"max_consumer_group_lag","timestamp":"2024-07-27T16:41:33Z",
# "value":"0","selector":null}]}
